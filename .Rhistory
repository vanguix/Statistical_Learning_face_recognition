D = pca_values$D
#2.2. KNN with PCA
#First, we select the data that will be used for KNN.
#The training set is already transformed in the previous PCA function.
datos_train = data.frame(train_PCA)
#The testing set needs to be transformed with the eigenvectors and the mean from
#the training set after applying PCA.
centered_data_test <- scale(test_matrix, center = means, scale = F)
#When applying the P eigenvectors
test_PCA <- centered_data_test %*% P
datos_test = data.frame(test_PCA)
knn_applied <- our_knn_multiple(train_PCA, test_PCA, target, friends=k, threshold= thres, metric=metric)
}
else{
#2.3. KNN without PCA
print('Option PCA not chosen.')
knn_applied <- our_knn_multiple(train_matrix, test_matrix, target, friends=k, threshold= thres, metric=metric)
}
return(knn_applied)
}
k_v = 1
th=40
var = 0.95
resultado <- classifier(train_matrix, test_matrix, 0, k = k_v, thres = th, metric = 'euclidean', var, target= df_train_data$target)
View(resultados_df)
a <- accuracy(resultado, df_test_data$target, df_test_data$split)
sum(resultado == as.character(df_test_data$target)) / length(resultado)
View(a)
print(a[1])
print(a)
k_v = 1
th=40
var = 0.95
resultado1 <- classifier(train_matrix, test_matrix, 1, k = k_v, thres = th, metric = 'euclidean', var, target= df_train_data$target)
a <- accuracy(resultado, df_test_data$target, df_test_data$split)
print(sum(resultado == as.character(df_test_data$target)) / length(resultado))
print(a)
k_v = 1
th=40
var = 0.95
resultado1 <- classifier(train_matrix, test_matrix, 1, k = k_v, thres = th, metric = 'euclidean', var, target= df_train_data$target)
a <- accuracy(resultado1, df_test_data$target, df_test_data$split)
print(sum(resultado == as.character(df_test_data$target)) / length(resultado))
print(a)
View(resultado1)
pca <- function(data, perc) {
# INPUT
# data - a matrix that contains the data to which we must apply the PCA (a set of observations)
# perc - a number that determines the percentage of variance that is wanted to be kept, with this parameter it is decided the number of principal componenets that are going to be kept
# OUTPUT
# result - a list that contains the following elements:
# mean - a vector that contains the mean of every column (this is useful to center the data)
# P - the rotation matrix (this is the matrix we must multiply our data to obtain the data reduced with PCA)
# D - a vector that contains the variance explained with every new eigenvector that is added
# reduced_data - the data with dimensionality reduction (that is having applied matrix P)
cat('PCA calculation begins for perc=', perc, '\n')
# Calculate the mean of the observations
mean_vec <- colMeans(data)
# Center the data by subtracting the mean (and we have to decide whether if we scale or not)
centered_data <- scale(data, center = mean_vec, scale = F) #cambiar centrer = T y ver si sale igual
# Compute the covariance matrix
cov_matrix <- cov(t(centered_data))
#From now on, we perform everything with the short form of the data matrix (as h<<P).
# Perform eigenvalue decomposition for the short
eigen_result <- eigen(cov_matrix) #is the short
# Extract eigenvectors (P) and eigenvalues (e_values)
e_values <- eigen_result$values #as P will be calculated using the shot and the eigenvalues of the long and the short are the same, we direclty calculate them with the short
P_short <- eigen_result$vectors #this cannot be calculated like this as data is too large (36000*36000) so instead we use the short (n x n) (n - number of instances in the data)
# Percentage of the variance retaining by the PCs
D<-cumsum(e_values)/sum(e_values) # vector length n
# Find the index of the first term in the vector D that retains the percentage of variance desired
#index <- which(D >= perc)[1]
index <- sum(D <= perc)+1
# Retain only the elements until that index and rewrite the vector D and the matrix P until that index
D <- D[1:index] # vector of length m (m<n or m=n)
P_short <- P_short[,1:index] # matrix dim n * m
#Now we calculate the eigenvectors of the long matrix
#to do this, we need the eigenvectors of the short and data
#When applying the P eigenvectors
P <- t(centered_data)%*%P_short #dim(P) 108000 * m
reduced_data<-centered_data%*% P # dim(reduced_data) n * m
# Return mean, eigenvectors (matrix P), and variance (matrix D) of the set of observations
result <- list(mean = mean_vec, P = P, D = D, reduced_data= reduced_data)
cat('PCA calculation finished. Means=', means,'eigenvectors=', P, 'Perc variance=', D)
return(result)
}
k_v = 1
th=40
var = 0.95
resultado1 <- classifier(train_matrix, test_matrix, 1, k = k_v, thres = th, metric = 'euclidean', var, target= df_train_data$target)
pca <- function(data, perc) {
# INPUT
# data - a matrix that contains the data to which we must apply the PCA (a set of observations)
# perc - a number that determines the percentage of variance that is wanted to be kept, with this parameter it is decided the number of principal componenets that are going to be kept
# OUTPUT
# result - a list that contains the following elements:
# mean - a vector that contains the mean of every column (this is useful to center the data)
# P - the rotation matrix (this is the matrix we must multiply our data to obtain the data reduced with PCA)
# D - a vector that contains the variance explained with every new eigenvector that is added
# reduced_data - the data with dimensionality reduction (that is having applied matrix P)
cat('PCA calculation begins for perc=', perc, '\n')
# Calculate the mean of the observations
mean_vec <- colMeans(data)
# Center the data by subtracting the mean (and we have to decide whether if we scale or not)
centered_data <- scale(data, center = mean_vec, scale = F) #cambiar centrer = T y ver si sale igual
# Compute the covariance matrix
cov_matrix <- cov(t(centered_data))
#From now on, we perform everything with the short form of the data matrix (as h<<P).
# Perform eigenvalue decomposition for the short
eigen_result <- eigen(cov_matrix) #is the short
# Extract eigenvectors (P) and eigenvalues (e_values)
e_values <- eigen_result$values #as P will be calculated using the shot and the eigenvalues of the long and the short are the same, we direclty calculate them with the short
P_short <- eigen_result$vectors #this cannot be calculated like this as data is too large (36000*36000) so instead we use the short (n x n) (n - number of instances in the data)
# Percentage of the variance retaining by the PCs
D<-cumsum(e_values)/sum(e_values) # vector length n
# Find the index of the first term in the vector D that retains the percentage of variance desired
#index <- which(D >= perc)[1]
index <- sum(D <= perc)+1
# Retain only the elements until that index and rewrite the vector D and the matrix P until that index
D <- D[1:index] # vector of length m (m<n or m=n)
P_short <- P_short[,1:index] # matrix dim n * m
#Now we calculate the eigenvectors of the long matrix
#to do this, we need the eigenvectors of the short and data
#When applying the P eigenvectors
P <- t(centered_data)%*%P_short #dim(P) 108000 * m
reduced_data<-centered_data%*% P # dim(reduced_data) n * m
# Return mean, eigenvectors (matrix P), and variance (matrix D) of the set of observations
result <- list(mean = mean_vec, P = P, D = D, reduced_data= reduced_data)
cat('PCA calculation finished. Means=', mean_vec,'eigenvectors=', P, 'Perc variance=', D)
return(result)
}
k_v = 1
th=40
var = 0.95
resultado1 <- classifier(train_matrix, test_matrix, 1, k = k_v, thres = th, metric = 'euclidean', var, target= df_train_data$target)
pca <- function(data, perc) {
# INPUT
# data - a matrix that contains the data to which we must apply the PCA (a set of observations)
# perc - a number that determines the percentage of variance that is wanted to be kept, with this parameter it is decided the number of principal componenets that are going to be kept
# OUTPUT
# result - a list that contains the following elements:
# mean - a vector that contains the mean of every column (this is useful to center the data)
# P - the rotation matrix (this is the matrix we must multiply our data to obtain the data reduced with PCA)
# D - a vector that contains the variance explained with every new eigenvector that is added
# reduced_data - the data with dimensionality reduction (that is having applied matrix P)
cat('PCA calculation begins for perc=', perc, '\n')
# Calculate the mean of the observations
mean_vec <- colMeans(data)
# Center the data by subtracting the mean (and we have to decide whether if we scale or not)
centered_data <- scale(data, center = mean_vec, scale = F) #cambiar centrer = T y ver si sale igual
# Compute the covariance matrix
cov_matrix <- cov(t(centered_data))
#From now on, we perform everything with the short form of the data matrix (as h<<P).
# Perform eigenvalue decomposition for the short
eigen_result <- eigen(cov_matrix) #is the short
# Extract eigenvectors (P) and eigenvalues (e_values)
e_values <- eigen_result$values #as P will be calculated using the shot and the eigenvalues of the long and the short are the same, we direclty calculate them with the short
P_short <- eigen_result$vectors #this cannot be calculated like this as data is too large (36000*36000) so instead we use the short (n x n) (n - number of instances in the data)
# Percentage of the variance retaining by the PCs
D<-cumsum(e_values)/sum(e_values) # vector length n
# Find the index of the first term in the vector D that retains the percentage of variance desired
#index <- which(D >= perc)[1]
index <- sum(D <= perc)+1
# Retain only the elements until that index and rewrite the vector D and the matrix P until that index
D <- D[1:index] # vector of length m (m<n or m=n)
P_short <- P_short[,1:index] # matrix dim n * m
#Now we calculate the eigenvectors of the long matrix
#to do this, we need the eigenvectors of the short and data
#When applying the P eigenvectors
P <- t(centered_data)%*%P_short #dim(P) 108000 * m
reduced_data<-centered_data%*% P # dim(reduced_data) n * m
# Return mean, eigenvectors (matrix P), and variance (matrix D) of the set of observations
result <- list(mean = mean_vec, P = P, D = D, reduced_data= reduced_data)
cat('PCA calculation finished. Eigenvectors=', P, 'Perc variance=', D)
return(result)
}
k_v = 1
th=40
var = 0.95
resultado1 <- classifier(train_matrix, test_matrix, 1, k = k_v, thres = th, metric = 'euclidean', var, target= df_train_data$target)
a <- accuracy(resultado1, df_test_data$target, df_test_data$split)
print(sum(resultado == as.character(df_test_data$target)) / length(resultado))
print(a)
pca <- function(data, perc) {
# INPUT
# data - a matrix that contains the data to which we must apply the PCA (a set of observations)
# perc - a number that determines the percentage of variance that is wanted to be kept, with this parameter it is decided the number of principal componenets that are going to be kept
# OUTPUT
# result - a list that contains the following elements:
# mean - a vector that contains the mean of every column (this is useful to center the data)
# P - the rotation matrix (this is the matrix we must multiply our data to obtain the data reduced with PCA)
# D - a vector that contains the variance explained with every new eigenvector that is added
# reduced_data - the data with dimensionality reduction (that is having applied matrix P)
cat('PCA calculation begins for perc=', perc, '\n')
# Calculate the mean of the observations
mean_vec <- colMeans(data)
# Center the data by subtracting the mean (and we have to decide whether if we scale or not)
centered_data <- scale(data, center = mean_vec, scale = F) #cambiar centrer = T y ver si sale igual
# Compute the covariance matrix
cov_matrix <- cov(t(centered_data))
#From now on, we perform everything with the short form of the data matrix (as h<<P).
# Perform eigenvalue decomposition for the short
eigen_result <- eigen(cov_matrix) #is the short
# Extract eigenvectors (P) and eigenvalues (e_values)
e_values <- eigen_result$values #as P will be calculated using the shot and the eigenvalues of the long and the short are the same, we direclty calculate them with the short
P_short <- eigen_result$vectors #this cannot be calculated like this as data is too large (36000*36000) so instead we use the short (n x n) (n - number of instances in the data)
# Percentage of the variance retaining by the PCs
D<-cumsum(e_values)/sum(e_values) # vector length n
# Find the index of the first term in the vector D that retains the percentage of variance desired
#index <- which(D >= perc)[1]
index <- sum(D <= perc)+1
# Retain only the elements until that index and rewrite the vector D and the matrix P until that index
D <- D[1:index] # vector of length m (m<n or m=n)
P_short <- P_short[,1:index] # matrix dim n * m
#Now we calculate the eigenvectors of the long matrix
#to do this, we need the eigenvectors of the short and data
#When applying the P eigenvectors
P <- t(centered_data)%*%P_short #dim(P) 108000 * m
reduced_data<-centered_data%*% P # dim(reduced_data) n * m
# Return mean, eigenvectors (matrix P), and variance (matrix D) of the set of observations
result <- list(mean = mean_vec, P = P, D = D, reduced_data= reduced_data)
cat('PCA calculation finished. Perc variance=', D)
return(result)
}
k_v = 1
th=40
var = 0.95
resultado1 <- classifier(train_matrix, test_matrix, 1, k = k_v, thres = th, metric = 'euclidean', var, target= df_train_data$target)
a <- accuracy(resultado1, df_test_data$target, df_test_data$split)
print(sum(resultado == as.character(df_test_data$target)) / length(resultado))
print(a)
k_v = 1
th=40
var = 0.95
resultado1 <- classifier(train_matrix, test_matrix, 1, k = k_v, thres = th, metric = 'euclidean', var, target= df_train_data$target)
a <- accuracy(resultado1, df_test_data$target, df_test_data$split)
print(sum(resultado1 == as.character(df_test_data$target)) / length(resultado))
print(a)
pca <- function(data, perc) {
# INPUT
# data - a matrix that contains the data to which we must apply the PCA (a set of observations)
# perc - a number that determines the percentage of variance that is wanted to be kept, with this parameter it is decided the number of principal componenets that are going to be kept
# OUTPUT
# result - a list that contains the following elements:
# mean - a vector that contains the mean of every column (this is useful to center the data)
# P - the rotation matrix (this is the matrix we must multiply our data to obtain the data reduced with PCA)
# D - a vector that contains the variance explained with every new eigenvector that is added
# reduced_data - the data with dimensionality reduction (that is having applied matrix P)
cat('PCA calculation begins for perc=', perc, '\n')
# Calculate the mean of the observations
mean_vec <- colMeans(data)
# Center the data by subtracting the mean (and we have to decide whether if we scale or not)
centered_data <- scale(data, center = mean_vec, scale = F) #cambiar centrer = T y ver si sale igual
# Compute the covariance matrix
cov_matrix <- cov(t(centered_data))
#From now on, we perform everything with the short form of the data matrix (as h<<P).
# Perform eigenvalue decomposition for the short
eigen_result <- eigen(cov_matrix) #is the short
# Extract eigenvectors (P) and eigenvalues (e_values)
e_values <- eigen_result$values #as P will be calculated using the shot and the eigenvalues of the long and the short are the same, we direclty calculate them with the short
P_short <- eigen_result$vectors #this cannot be calculated like this as data is too large (36000*36000) so instead we use the short (n x n) (n - number of instances in the data)
# Percentage of the variance retaining by the PCs
D<-cumsum(e_values)/sum(e_values) # vector length n
# Find the index of the first term in the vector D that retains the percentage of variance desired
#index <- which(D >= perc)[1]
index <- sum(D <= perc)+1
# Retain only the elements until that index and rewrite the vector D and the matrix P until that index
D <- D[1:index] # vector of length m (m<n or m=n)
P_short <- P_short[,1:index] # matrix dim n * m
#Now we calculate the eigenvectors of the long matrix
#to do this, we need the eigenvectors of the short and data
#When applying the P eigenvectors
P <- t(centered_data)%*%P_short #dim(P) 108000 * m
reduced_data<-centered_data%*% P # dim(reduced_data) n * m
# Return mean, eigenvectors (matrix P), and variance (matrix D) of the set of observations
result <- list(mean = mean_vec, P = P, D = D, reduced_data= reduced_data)
cat('PCA calculation finished. Perc variance=', D, 'index=', index)
return(result)
}
k_v = 1
th=40
var = 0.95
resultado1 <- classifier(train_matrix, test_matrix, 1, k = k_v, thres = th, metric = 'euclidean', var, target= df_train_data$target)
a <- accuracy(resultado1, df_test_data$target, df_test_data$split)
print(sum(resultado1 == as.character(df_test_data$target)) / length(resultado))
print(a)
k_v = 1
th=5
var = 0.95
resultado1 <- classifier(train_matrix, test_matrix, 1, k = k_v, thres = th, metric = 'euclidean', var, target= df_train_data$target)
a <- accuracy(resultado1, df_test_data$target, df_test_data$split)
print(sum(resultado1 == as.character(df_test_data$target)) / length(resultado))
print(a)
classifier <- function(train_matrix, test_matrix,PCAopt,k,thres,metric='manhattan', perc, target) {
# INPUT
# df_train_data - the data (without the label) of the training set (it is a matrix)
# df_test_data - the data (without the label) of the test set (it is a matrix)
# PCAopt - binary value 1 if we want to apply PCA, 0 if we do not want to apply PCA
# k - number of neighbours for the KNN method
# thres - threshold value to determine whether a new image belongs to the dataset or not
# OUTPUT
# it returns the output of the knn applied
#1. Read the files from the dataframe
#images_train = read_images(df_train_data$file)
#images_test = read_images(df_test_data$file)
#1. Transform the data
#train_matrix = transform_data(images_train)
#test_matrix = transform_data(images_test)
#print('----------------')
#print(dim(train_matrix))
#print(dim(test_matrix))
#2. Apply KNN
if (PCAopt == TRUE) {
#2.1. Apply the PCA function only to the training
cat('Option PCA chosen. Calculation begins for: k= ', k, ',Th=', thres, ', metric=', metric, ', perc=', perc,  '\n')
pca_values = pca(train_matrix, perc)
P= pca_values$P
means = pca_values$mean
train_PCA =  pca_values$reduced_data #train data having used PCA to reduce dimensionality
D = pca_values$D
#2.2. KNN with PCA
#First, we select the data that will be used for KNN.
#The training set is already transformed in the previous PCA function.
datos_train = data.frame(train_PCA)
#The testing set needs to be transformed with the eigenvectors and the mean from
#the training set after applying PCA.
centered_data_test <- scale(test_matrix, center = means, scale = F)
#When applying the P eigenvectors
test_PCA <- centered_data_test %*% P
datos_test = data.frame(test_PCA)
knn_applied <- our_knn_multiple(datos_train, datos_test, target, friends=k, threshold= thres, metric=metric)
}
else{
#2.3. KNN without PCA
print('Option PCA not chosen.')
knn_applied <- our_knn_multiple(train_matrix, test_matrix, target, friends=k, threshold= thres, metric=metric)
}
return(knn_applied)
}
k_v = 1
th=5
var = 0.95
resultado1 <- classifier(train_matrix, test_matrix, 1, k = k_v, thres = th, metric = 'euclidean', var, target= df_train_data$target)
a <- accuracy(resultado1, df_test_data$target, df_test_data$split)
print(sum(resultado1 == as.character(df_test_data$target)) / length(resultado))
print(a)
k_v = 1
th=5
var = 0.8
resultado1 <- classifier(train_matrix, test_matrix, 1, k = k_v, thres = th, metric = 'euclidean', var, target= df_train_data$target)
a <- accuracy(resultado1, df_test_data$target, df_test_data$split)
print(sum(resultado1 == as.character(df_test_data$target)) / length(resultado))
print(a)
k_v = 1
th=50000
var = 0.8
resultado1 <- classifier(train_matrix, test_matrix, 1, k = k_v, thres = th, metric = 'euclidean', var, target= df_train_data$target)
a <- accuracy(resultado1, df_test_data$target, df_test_data$split)
print(sum(resultado1 == as.character(df_test_data$target)) / length(resultado))
print(a)
k_v = 1
th=90
var = 0.95
resultado1 <- classifier(train_matrix, test_matrix, 1, k = k_v, thres = th, metric = 'euclidean', var, target= df_train_data$target)
a <- accuracy(resultado1, df_test_data$target, df_test_data$split)
print(sum(resultado1 == as.character(df_test_data$target)) / length(resultado))
print(a)
pca_values = pca(train_matrix, perc)
pca_values = pca(train_matrix, var)
train_PCA =  pca_values$reduced_data
# Compute the distances between instances
distance_matrix= dist(train_PCA, method = 'euclidean')
# Plot a heatmap
heatmap(as.matrix(distance_matrix),
main = paste("Heatmap for ", metric, " distance"),
col= heat.colors(11))
# Obtener los valores únicos en la matriz
legend("topright", legend = seq(min(distance_matrix), max(distance_matrix), length.out = 11),fill = heat.colors(11), title = "Values", cex = 0.8)
pca_values = pca(train_matrix, var)
train_PCA =  pca_values$reduced_data
metrics = list('euclidean','manhattan','maximum')
for (metric in metrics) {
# Compute the distances between instances
distance_matrix= dist(train_PCA, method = metric)
# Plot a heatmap
heatmap(as.matrix(distance_matrix),
main = paste("Heatmap for ", metric, " distance"),
col= heat.colors(11))
# Obtener los valores únicos en la matriz
legend("topright", legend = seq(min(distance_matrix), max(distance_matrix), length.out = 11),fill = heat.colors(11), title = "Values", cex = 0.8)
}
View(a)
# Inicializar un dataframe para almacenar los resultados
resultados_df_euclidean <- data.frame(metric = character(),
threshold = numeric(),
percentage_var = numeric(),
k_value = numeric(),
accuracy = numeric(),
stringsAsFactors = FALSE)
# Compute the distances between instances
distance_matrix= distance_matrices[['euclidean']]
var= 0.95
pca_values = pca(train_matrix, var)
train_PCA =  pca_values$reduced_data
metrics = list('euclidean','manhattan','maximum')
distance_matrices <- list()
for (metric in metrics) {
# Compute the distances between instances
distance_matrix= dist(train_PCA, method = metric)
# Store the distance matrix in the list
distance_matrices[[metric]] <- distance_matrix
# Plot a heatmap
heatmap(as.matrix(distance_matrix),
main = paste("Heatmap for ", metric, " distance"),
col= heat.colors(11))
# Obtener los valores únicos en la matriz
legend("topright", legend = seq(min(distance_matrix), max(distance_matrix), length.out = 11),fill = heat.colors(11), title = "Values", cex = 0.8)
}
# Divide the set of images into a test set and train set
prueba2 = split_train_test(files_list)
# Take the train set and the test set in sepparate dataframes
df_train_data <- subset(prueba2,split == 'train')
df_test_data <- subset(prueba2,split != 'train')
#1. Read the files from the dataframe
images_train = read_images(df_train_data$file)
images_test = read_images(df_test_data$file)
#1. Transform the data
train_matrix = transform_data(images_train)
test_matrix = transform_data(images_test)
# Inicializar un dataframe para almacenar los resultados
resultados_df_euclidean <- data.frame(metric = character(),
threshold = numeric(),
percentage_var = numeric(),
k_value = numeric(),
accuracy = numeric(),
stringsAsFactors = FALSE)
# Compute the distances between instances
distance_matrix= distance_matrices[['euclidean']]
thresholds = seq(min(distance_matrix), max(distance_matrix), length.out = 11)
percentage_var= list(0.85,0.90,0.95)
k_values = list(1,3,5) #we choose odd numbers to avoid ties
for (th in thresholds) {
for (var in percentage_var) {
for (k_v in k_values) {
resultado <- classifier(train_matrix, test_matrix, 1, k = k_v, thres = th, metric = 'euclidean', var)
a <- accuracy(resultado, df_test_data$target, df_test_data$split)
# Almacenar los resultados en el dataframe
resultados_df_euclidean <- bind_rows(resultados_df, data.frame(metric = "euclidean", threshold = th, percentage_var = var, k_value = k_v, accuracy = a[2]))
}
}
}
# Inicializar un dataframe para almacenar los resultados
resultados_df_euclidean <- data.frame(metric = character(),
threshold = numeric(),
percentage_var = numeric(),
k_value = numeric(),
accuracy = numeric(),
stringsAsFactors = FALSE)
# Compute the distances between instances
distance_matrix= distance_matrices[['euclidean']]
thresholds = seq(min(distance_matrix), max(distance_matrix), length.out = 11)
percentage_var= list(0.85,0.90,0.95)
k_values = list(1,3,5) #we choose odd numbers to avoid ties
for (th in thresholds) {
for (var in percentage_var) {
for (k_v in k_values) {
resultado <- classifier(train_matrix, test_matrix, 1, k = k_v, thres = th, metric = 'euclidean', var, df_test_data$target)
a <- accuracy(resultado, df_test_data$target, df_test_data$split)
# Almacenar los resultados en el dataframe
resultados_df_euclidean <- bind_rows(resultados_df, data.frame(metric = "euclidean", threshold = th, percentage_var = var, k_value = k_v, accuracy = a[2]))
}
}
}
View(resultados_df_euclidean)
# Include the library OpenImageR that is required to read the images
library(OpenImageR)
read_images <- function(lista_archivos){
# INPUT
# file_list - a list with all the paths of the images
# OUTPUT
# a list with all the data of the images (each item of the list corresponds to one image)
# Create an empty list to store data of the images
data_images <- list()
i=1
# Loop to read every image and store its data in the list
for (archivo in lista_archivos) {
data_image = readImage(archivo)
data_images[[i]] <- data_image
i=i + 1
}
print('Images readed.')
return(data_images)
}
# Include the library OpenImageR that is required to read the images
library(OpenImageR)
read_images <- function(lista_archivos){
# INPUT
# file_list - a list with all the paths of the images
# OUTPUT
# a list with all the data of the images (each item of the list corresponds to one image)
# Create an empty list to store data of the images
data_images <- list()
i=1
# Loop to read every image and store its data in the list
for (archivo in lista_archivos) {
data_image = readImage(archivo)
data_images[[i]] <- data_image
i=i + 1
}
print('Images readed.')
return(data_images)
}
