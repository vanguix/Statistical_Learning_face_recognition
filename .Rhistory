test_matrix = transform_data(images_test)
print('----------------')
print(dim(train_matrix))
print(dim(test_matrix))
if (PCAopt == TRUE) {
#2. Apply the PCA function only to the training
pca_values = pca(train_matrix)
means = pca_values$mean
train_PCA = pca_values$reduced_data #train data having used PCA to reduce dimensionality
D = pca_values$D
#3. KNN with PCA
#First, we select the data that will be used for KNN.
#The training set is already transformed in the previous PCA function.
datos_train = data.frame(x = train_PCA[,1], #taking the two first components of train_PCA
y = train_PCA[,2])
#The testing set needs to be transformed with the eigenvectors and the mean from
#the training set after applying PCA.
centered_data <- scale(test_matrix, center = means, scale = F)
#When applying the P eigenvectors
test_PCA <- centered_data%*%pca_values$P
datos_test = data.frame(x = test_PCA[,1], #taking the two first components of test_PCA
y = test_PCA[,2])
knn_applied <- our_knn(datos_train, datos_test, df_train_data$target, 3)
}
else{
#KNN without PCA
knn_applied <- our_knn(train_matrix, test_matrix, df_train_data$target, 3)
}
#identifier =
return(knn_applied)
}
parameters = list(var_expained=0.9,k=9,metric='mse',threshold=3)
resultado = classifier(df_train_data, df_test_data, parameters, 1) #using PCA
resultado = classifier(df_train_data, df_test_data, parameters, 0) #using PCA
View(resultado)
#DATA IS A SET OF OBSERVATIONS
pca <- function(data) {
# Calculate the mean of the observations
mean_vec <- colMeans(data)
# Center the data by subtracting the mean (and we have to decide whether if we scale or not)
centered_data <- scale(data, center = mean_vec, scale = F) #cambiar centrer = T y ver si sale igual
# Compute the covariance matrix
cov_matrix <- cov.wt(centered_data)
#From now on, we perform everything with the short form of the data matrix (as h<<P).
# Perform eigenvalue decomposition for the short
eigen_result <- eigen(t(cov_matrix)) #is the short this transposed?
# Extract eigenvectors (P) and eigenvalues (e_values)
e_values <- eigen_result$values #as P will be calculated using the shot and the eigenvalues of the long and the short are the same, we direclty calculate them with the short
P_short <- eigen_result$vectors #this cannot be calculated like this as data is too large (36000*36000) so instead we use the short (150x150)
print(dim(P_short)) # should be 150*1?????????????????????????
print(dim(data))
sdev <- sqrt(e_values) #standard deviations of the principal components
#Percentage of the variance retaining by the PCs
D <- cumsum(sdev^2/sum(sdev^2))
#Now we calculate the eigenvectors of the long matrix
#to do this, we need the eigenvectors of the short and data
#When applying the P eigenvectors
reduced_data <- centered_data%*%P_short #shape(P) should be 36000*1
# Return mean, eigenvectors (matrix P), and variance (matrix D) of the set of observations
result <- list(mean = mean_vec, P = P_short, D = D, reduced_data = reduced_data)
return(result)
}
resultado = classifier(df_train_data, df_test_data, parameters, 1) #using PCA
#DATA IS A SET OF OBSERVATIONS
pca <- function(data) {
# Calculate the mean of the observations
mean_vec <- colMeans(data)
# Center the data by subtracting the mean (and we have to decide whether if we scale or not)
centered_data <- scale(data, center = mean_vec, scale = F) #cambiar centrer = T y ver si sale igual
# Compute the covariance matrix
cov_matrix <- cov.wt(centered_data)
#From now on, we perform everything with the short form of the data matrix (as h<<P).
# Perform eigenvalue decomposition for the short
eigen_result <- eigen(cov_matrix) #is the short this transposed?
# Extract eigenvectors (P) and eigenvalues (e_values)
e_values <- eigen_result$values #as P will be calculated using the shot and the eigenvalues of the long and the short are the same, we direclty calculate them with the short
P_short <- eigen_result$vectors #this cannot be calculated like this as data is too large (36000*36000) so instead we use the short (150x150)
print(dim(P_short)) # should be 150*1?????????????????????????
print(dim(data))
sdev <- sqrt(e_values) #standard deviations of the principal components
#Percentage of the variance retaining by the PCs
D <- cumsum(sdev^2/sum(sdev^2))
#Now we calculate the eigenvectors of the long matrix
#to do this, we need the eigenvectors of the short and data
#When applying the P eigenvectors
reduced_data <- centered_data%*%P_short #shape(P) should be 36000*1
# Return mean, eigenvectors (matrix P), and variance (matrix D) of the set of observations
result <- list(mean = mean_vec, P = P_short, D = D, reduced_data = reduced_data)
return(result)
}
classifier <- function(df_train_data, df_test_data, parameters, PCAopt) {
#1. Read the files from the dataframe
images_train = read_images(df_train_data$file)
images_test = read_images(df_test_data$file)
#1. Transform the data
train_matrix = transform_data(images_train)
test_matrix = transform_data(images_test)
print('----------------')
print(dim(train_matrix))
print(dim(test_matrix))
if (PCAopt == TRUE) {
#2. Apply the PCA function only to the training
pca_values = pca(train_matrix)
means = pca_values$mean
train_PCA = pca_values$reduced_data #train data having used PCA to reduce dimensionality
D = pca_values$D
#3. KNN with PCA
#First, we select the data that will be used for KNN.
#The training set is already transformed in the previous PCA function.
datos_train = data.frame(x = train_PCA[,1], #taking the two first components of train_PCA
y = train_PCA[,2])
#The testing set needs to be transformed with the eigenvectors and the mean from
#the training set after applying PCA.
centered_data <- scale(test_matrix, center = means, scale = F)
#When applying the P eigenvectors
test_PCA <- centered_data%*%pca_values$P
datos_test = data.frame(x = test_PCA[,1], #taking the two first components of test_PCA
y = test_PCA[,2])
knn_applied <- our_knn(datos_train, datos_test, df_train_data$target, 3)
}
else{
#KNN without PCA
knn_applied <- our_knn(train_matrix, test_matrix, df_train_data$target, 3)
}
#identifier =
return(knn_applied)
}
parameters = list(var_expained=0.9,k=9,metric='mse',threshold=3)
resultado = classifier(df_train_data, df_test_data, parameters, 1) #using PCA
library(OpenImageR)
# Install and load the imager package
#install.packages("imager")
#library(imager)
library(gridExtra)
library(grid)
library(class)
library(dplyr)
source("our_knn.R")
split_train_test <- function(files) {
set.seed(42) #To select people randomly but in the same way in every execution of the code
# Extract label from every image
images <- c()
identifiers <- c()
for (image in files) {
if (substr(image, nchar(image) - 3 + 1, nchar(image)) == 'jpg') {
images <- c(images, image)
label <- gsub(".*?([0-9]+).*", "\\1", image) #1 refers to first
identifiers <- c(identifiers, as.numeric(label)) #there should be 25 ids
}
}
# Set up the dataframe
df <- data.frame(images, as.factor(identifiers))
names(df) <- c('file', 'target')
# Randomly select 4 identifiers to exclude from training
set.seed(42)
excluded_identifiers <- sample(unique(df$target), 4)
# Exclude the selected identifiers from the training set
df_train_validation <- df[!df$target %in% excluded_identifiers,]
# Randomly select 5 images for training and 1 for testing for each remaining identifier
df_train_validation <- df_train_validation %>%
group_by(target) %>%
mutate(split = sample(c(rep('train', 5), 'test'), size = n())) %>% #5 IMAGES TO TRAIN, 1 to test
ungroup()
# Add the excluded identifiers to the test set
df_test <- df[df$target %in% excluded_identifiers,]
df_test$split <- 'test'
# Combine the datasets
split_df <- rbind(df_train_validation, df_test)
return(as.data.frame(split_df))
}
# Specify the directory containing your image files
#setwd("C:/Users/laram/Desktop/Todo/UC3M/Second Bimester/Statistical Learning/Assignment")
#image_directory <- "C:/Users/laram/Desktop/Todo/UC3M/Second Bimester/Statistical Learning/Assignment/Training"
image_directory <- "C:/Training"
# Obtén la lista de archivos en la carpeta
lista_archivos <- list.files(path = image_directory, pattern = ".*\\.jpg$", full.names = TRUE)
prueba2 = split_train_test(lista_archivos)
df_train_data <- subset(prueba2,split == 'train')
df_test_data <- subset(prueba2,split == 'test')
read_images <- function(lista_archivos){
# Crea una lista para almacenar las imágenes
data_images <- list()
i=1
# Bucle for para leer cada imagen
for (archivo in lista_archivos) {
# Leer la imagen y almacenarla en la lista
data_image = readImage(archivo)
data_images[[i]] <- data_image
i=i + 1
}
return(data_images)
}
transform_data <- function(image_list){
#Add every image to the matrix
# Inicializes the matrix
#pic_matrix <- matrix(nrow = 0, ncol = 3 * dim(image_list[[1]])[1])  # Inicializa con el tamaño correcto
pic_matrix <- matrix(nrow = 0, ncol = 108000)  # Inicializa con el tamaño correcto
# Transforma los datos de cada imagen y agrega a la matriz
for (i in seq_along(image_list)) {
pic <- image_list[[i]]
# Transforma los datos de las tres componentes en una fila de la futura matriz de datos
new_row <- t(as.vector(pic))
pic_matrix <- rbind(pic_matrix, new_row)
# Verifying that the necessary matrix has the correct dimensions
#print(dim(pic_matrix))
}
return(pic_matrix)
}
#DATA IS A SET OF OBSERVATIONS
pca <- function(data) {
# Calculate the mean of the observations
mean_vec <- colMeans(data)
# Center the data by subtracting the mean (and we have to decide whether if we scale or not)
centered_data <- scale(data, center = mean_vec, scale = F) #cambiar centrer = T y ver si sale igual
# Compute the covariance matrix
cov_matrix <- cov.wt(centered_data)
#From now on, we perform everything with the short form of the data matrix (as h<<P).
# Perform eigenvalue decomposition for the short
eigen_result <- eigen(cov_matrix) #is the short this transposed?
# Extract eigenvectors (P) and eigenvalues (e_values)
e_values <- eigen_result$values #as P will be calculated using the shot and the eigenvalues of the long and the short are the same, we direclty calculate them with the short
P_short <- eigen_result$vectors #this cannot be calculated like this as data is too large (36000*36000) so instead we use the short (150x150)
print(dim(P_short)) # should be 150*1?????????????????????????
print(dim(data))
sdev <- sqrt(e_values) #standard deviations of the principal components
#Percentage of the variance retaining by the PCs
D <- cumsum(sdev^2/sum(sdev^2))
#Now we calculate the eigenvectors of the long matrix
#to do this, we need the eigenvectors of the short and data
#When applying the P eigenvectors
reduced_data <- centered_data%*%P_short #shape(P) should be 36000*1
# Return mean, eigenvectors (matrix P), and variance (matrix D) of the set of observations
result <- list(mean = mean_vec, P = P_short, D = D, reduced_data = reduced_data)
return(result)
}
classifier <- function(df_train_data, df_test_data, parameters, PCAopt) {
#1. Read the files from the dataframe
images_train = read_images(df_train_data$file)
images_test = read_images(df_test_data$file)
#1. Transform the data
train_matrix = transform_data(images_train)
test_matrix = transform_data(images_test)
print('----------------')
print(dim(train_matrix))
print(dim(test_matrix))
if (PCAopt == TRUE) {
#2. Apply the PCA function only to the training
pca_values = pca(train_matrix)
means = pca_values$mean
train_PCA = pca_values$reduced_data #train data having used PCA to reduce dimensionality
D = pca_values$D
#3. KNN with PCA
#First, we select the data that will be used for KNN.
#The training set is already transformed in the previous PCA function.
datos_train = data.frame(x = train_PCA[,1], #taking the two first components of train_PCA
y = train_PCA[,2])
#The testing set needs to be transformed with the eigenvectors and the mean from
#the training set after applying PCA.
centered_data <- scale(test_matrix, center = means, scale = F)
#When applying the P eigenvectors
test_PCA <- centered_data%*%pca_values$P
datos_test = data.frame(x = test_PCA[,1], #taking the two first components of test_PCA
y = test_PCA[,2])
knn_applied <- our_knn(datos_train, datos_test, df_train_data$target, 3)
}
else{
#KNN without PCA
knn_applied <- our_knn(train_matrix, test_matrix, df_train_data$target, 3)
}
#identifier =
return(knn_applied)
}
parameters = list(var_expained=0.9,k=9,metric='mse',threshold=3)
resultado = classifier(df_train_data, df_test_data, parameters, 1) #using PCA
#DATA IS A SET OF OBSERVATIONS
pca <- function(data) {
# Calculate the mean of the observations
mean_vec <- colMeans(data)
# Center the data by subtracting the mean (and we have to decide whether if we scale or not)
centered_data <- scale(data, center = mean_vec, scale = F) #cambiar centrer = T y ver si sale igual
# Compute the covariance matrix
cov_matrix <- cov.wt(centered_data)
#From now on, we perform everything with the short form of the data matrix (as h<<P).
# Perform eigenvalue decomposition for the short
eigen_result <- eigen.wt(t(cov_matrix)) #is the short this transposed?
# Extract eigenvectors (P) and eigenvalues (e_values)
e_values <- eigen_result$values #as P will be calculated using the shot and the eigenvalues of the long and the short are the same, we direclty calculate them with the short
P_short <- eigen_result$vectors #this cannot be calculated like this as data is too large (36000*36000) so instead we use the short (150x150)
print(dim(P_short)) # should be 150*1?????????????????????????
print(dim(data))
sdev <- sqrt(e_values) #standard deviations of the principal components
#Percentage of the variance retaining by the PCs
D <- cumsum(sdev^2/sum(sdev^2))
#Now we calculate the eigenvectors of the long matrix
#to do this, we need the eigenvectors of the short and data
#When applying the P eigenvectors
reduced_data <- centered_data%*%P_short #shape(P) should be 36000*1
# Return mean, eigenvectors (matrix P), and variance (matrix D) of the set of observations
result <- list(mean = mean_vec, P = P_short, D = D, reduced_data = reduced_data)
return(result)
}
#DATA IS A SET OF OBSERVATIONS
pca <- function(data) {
# Calculate the mean of the observations
mean_vec <- colMeans(data)
# Center the data by subtracting the mean (and we have to decide whether if we scale or not)
centered_data <- scale(data, center = mean_vec, scale = F) #cambiar centrer = T y ver si sale igual
# Compute the covariance matrix
cov_matrix <- cov.wt(centered_data)
#From now on, we perform everything with the short form of the data matrix (as h<<P).
# Perform eigenvalue decomposition for the short
eigen_result <- eigen.wt(t(cov_matrix)) #is the short this transposed?
# Extract eigenvectors (P) and eigenvalues (e_values)
e_values <- eigen_result$values #as P will be calculated using the shot and the eigenvalues of the long and the short are the same, we direclty calculate them with the short
P_short <- eigen_result$vectors #this cannot be calculated like this as data is too large (36000*36000) so instead we use the short (150x150)
print(dim(P_short)) # should be 150*1?????????????????????????
print(dim(data))
sdev <- sqrt(e_values) #standard deviations of the principal components
#Percentage of the variance retaining by the PCs
D <- cumsum(sdev^2/sum(sdev^2))
#Now we calculate the eigenvectors of the long matrix
#to do this, we need the eigenvectors of the short and data
#When applying the P eigenvectors
reduced_data <- centered_data%*%P_short #shape(P) should be 36000*1
# Return mean, eigenvectors (matrix P), and variance (matrix D) of the set of observations
result <- list(mean = mean_vec, P = P_short, D = D, reduced_data = reduced_data)
return(result)
}
classifier <- function(df_train_data, df_test_data, parameters, PCAopt) {
#1. Read the files from the dataframe
images_train = read_images(df_train_data$file)
images_test = read_images(df_test_data$file)
#1. Transform the data
train_matrix = transform_data(images_train)
test_matrix = transform_data(images_test)
print('----------------')
print(dim(train_matrix))
print(dim(test_matrix))
if (PCAopt == TRUE) {
#2. Apply the PCA function only to the training
pca_values = pca(train_matrix)
means = pca_values$mean
train_PCA = pca_values$reduced_data #train data having used PCA to reduce dimensionality
D = pca_values$D
#3. KNN with PCA
#First, we select the data that will be used for KNN.
#The training set is already transformed in the previous PCA function.
datos_train = data.frame(x = train_PCA[,1], #taking the two first components of train_PCA
y = train_PCA[,2])
#The testing set needs to be transformed with the eigenvectors and the mean from
#the training set after applying PCA.
centered_data <- scale(test_matrix, center = means, scale = F)
#When applying the P eigenvectors
test_PCA <- centered_data%*%pca_values$P
datos_test = data.frame(x = test_PCA[,1], #taking the two first components of test_PCA
y = test_PCA[,2])
knn_applied <- our_knn(datos_train, datos_test, df_train_data$target, 3)
}
else{
#KNN without PCA
knn_applied <- our_knn(train_matrix, test_matrix, df_train_data$target, 3)
}
#identifier =
return(knn_applied)
}
parameters = list(var_expained=0.9,k=9,metric='mse',threshold=3)
resultado = classifier(df_train_data, df_test_data, parameters, 1) #using PCA
library(OpenImageR)
# Install and load the imager package
#install.packages("imager")
#library(imager)
library(gridExtra)
library(grid)
library(class)
library(dplyr)
source("our_knn.R")
split_train_test <- function(files) {
set.seed(42) #To select people randomly but in the same way in every execution of the code
# Extract label from every image
images <- c()
identifiers <- c()
for (image in files) {
if (substr(image, nchar(image) - 3 + 1, nchar(image)) == 'jpg') {
images <- c(images, image)
label <- gsub(".*?([0-9]+).*", "\\1", image) #1 refers to first
identifiers <- c(identifiers, as.numeric(label)) #there should be 25 ids
}
}
# Set up the dataframe
df <- data.frame(images, as.factor(identifiers))
names(df) <- c('file', 'target')
# Randomly select 4 identifiers to exclude from training
set.seed(42)
excluded_identifiers <- sample(unique(df$target), 4)
# Exclude the selected identifiers from the training set
df_train_validation <- df[!df$target %in% excluded_identifiers,]
# Randomly select 5 images for training and 1 for testing for each remaining identifier
df_train_validation <- df_train_validation %>%
group_by(target) %>%
mutate(split = sample(c(rep('train', 5), 'test'), size = n())) %>% #5 IMAGES TO TRAIN, 1 to test
ungroup()
# Add the excluded identifiers to the test set
df_test <- df[df$target %in% excluded_identifiers,]
df_test$split <- 'test'
# Combine the datasets
split_df <- rbind(df_train_validation, df_test)
return(as.data.frame(split_df))
}
# Specify the directory containing your image files
#setwd("C:/Users/laram/Desktop/Todo/UC3M/Second Bimester/Statistical Learning/Assignment")
#image_directory <- "C:/Users/laram/Desktop/Todo/UC3M/Second Bimester/Statistical Learning/Assignment/Training"
image_directory <- "C:/Training"
# Obtén la lista de archivos en la carpeta
lista_archivos <- list.files(path = image_directory, pattern = ".*\\.jpg$", full.names = TRUE)
prueba2 = split_train_test(lista_archivos)
df_train_data <- subset(prueba2,split == 'train')
df_test_data <- subset(prueba2,split == 'test')
read_images <- function(lista_archivos){
# Crea una lista para almacenar las imágenes
data_images <- list()
i=1
# Bucle for para leer cada imagen
for (archivo in lista_archivos) {
# Leer la imagen y almacenarla en la lista
data_image = readImage(archivo)
data_images[[i]] <- data_image
i=i + 1
}
return(data_images)
}
transform_data <- function(image_list){
#Add every image to the matrix
# Inicializes the matrix
#pic_matrix <- matrix(nrow = 0, ncol = 3 * dim(image_list[[1]])[1])  # Inicializa con el tamaño correcto
pic_matrix <- matrix(nrow = 0, ncol = 108000)  # Inicializa con el tamaño correcto
# Transforma los datos de cada imagen y agrega a la matriz
for (i in seq_along(image_list)) {
pic <- image_list[[i]]
# Transforma los datos de las tres componentes en una fila de la futura matriz de datos
new_row <- t(as.vector(pic))
pic_matrix <- rbind(pic_matrix, new_row)
# Verifying that the necessary matrix has the correct dimensions
#print(dim(pic_matrix))
}
return(pic_matrix)
}
#DATA IS A SET OF OBSERVATIONS
pca <- function(data) {
# Calculate the mean of the observations
mean_vec <- colMeans(data)
# Center the data by subtracting the mean (and we have to decide whether if we scale or not)
centered_data <- scale(data, center = mean_vec, scale = F) #cambiar centrer = T y ver si sale igual
# Compute the covariance matrix
cov_matrix <- cov.wt(centered_data)
#From now on, we perform everything with the short form of the data matrix (as h<<P).
# Perform eigenvalue decomposition for the short
eigen_result <- eigen.wt(t(cov_matrix)) #is the short this transposed?
# Extract eigenvectors (P) and eigenvalues (e_values)
e_values <- eigen_result$values #as P will be calculated using the shot and the eigenvalues of the long and the short are the same, we direclty calculate them with the short
P_short <- eigen_result$vectors #this cannot be calculated like this as data is too large (36000*36000) so instead we use the short (150x150)
print(dim(P_short)) # should be 150*1?????????????????????????
print(dim(data))
sdev <- sqrt(e_values) #standard deviations of the principal components
#Percentage of the variance retaining by the PCs
D <- cumsum(sdev^2/sum(sdev^2))
#Now we calculate the eigenvectors of the long matrix
#to do this, we need the eigenvectors of the short and data
#When applying the P eigenvectors
reduced_data <- centered_data%*%P_short #shape(P) should be 36000*1
# Return mean, eigenvectors (matrix P), and variance (matrix D) of the set of observations
result <- list(mean = mean_vec, P = P_short, D = D, reduced_data = reduced_data)
return(result)
}
classifier <- function(df_train_data, df_test_data, parameters, PCAopt) {
#1. Read the files from the dataframe
images_train = read_images(df_train_data$file)
images_test = read_images(df_test_data$file)
#1. Transform the data
train_matrix = transform_data(images_train)
test_matrix = transform_data(images_test)
print('----------------')
print(dim(train_matrix))
print(dim(test_matrix))
if (PCAopt == TRUE) {
#2. Apply the PCA function only to the training
pca_values = pca(train_matrix)
means = pca_values$mean
train_PCA = pca_values$reduced_data #train data having used PCA to reduce dimensionality
D = pca_values$D
#3. KNN with PCA
#First, we select the data that will be used for KNN.
#The training set is already transformed in the previous PCA function.
datos_train = data.frame(x = train_PCA[,1], #taking the two first components of train_PCA
y = train_PCA[,2])
#The testing set needs to be transformed with the eigenvectors and the mean from
#the training set after applying PCA.
centered_data <- scale(test_matrix, center = means, scale = F)
#When applying the P eigenvectors
test_PCA <- centered_data%*%pca_values$P
datos_test = data.frame(x = test_PCA[,1], #taking the two first components of test_PCA
y = test_PCA[,2])
knn_applied <- our_knn(datos_train, datos_test, df_train_data$target, 3)
}
else{
#KNN without PCA
knn_applied <- our_knn(train_matrix, test_matrix, df_train_data$target, 3)
}
#identifier =
return(knn_applied)
}
parameters = list(var_expained=0.9,k=9,metric='mse',threshold=3)
resultado = classifier(df_train_data, df_test_data, parameters, 1) #using PCA
